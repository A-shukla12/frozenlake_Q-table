{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training RL-agent to play FrozenLake\n",
        "By: Akshara Shukla\n",
        "\n",
        "In this script, I will be walking though the steps involved in training an agent to play the FrozenLake game by using Q-table and learning parameters of reinforcement learning. The tutorial I have followed is available on this [link](https://www.youtube.com/watch?v=HGeI30uATws&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&index=10&ab_channel=deeplizard)."
      ],
      "metadata": {
        "id": "skpOiNxq6PCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Importing the required libraries"
      ],
      "metadata": {
        "id": "iIx5iXiP6oHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6-16k8HN-Re"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import gym \n",
        "import random \n",
        "import time \n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIhjGiXinKHQ",
        "outputId": "9b3e8aaa-0553-4a10-ee11-5bc837bea88b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement openai-gym (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for openai-gym\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "metadata": {
        "id": "A57jLhtivTNm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym[toy_text]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG2iPmTTyiFP",
        "outputId": "5d5c2892-3600-4cbd-b462-9fd79547a5e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[toy_text]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 95 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[toy_text]) (4.1.1)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "import pygame\n",
        "pygame.init()\n",
        "screen = pygame.display.set_mode((400, 300))"
      ],
      "metadata": {
        "id": "rqCJ1rVxqXhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Importing the game using the gym library enviornment"
      ],
      "metadata": {
        "id": "qGZ9fM4R6uGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSJzmftJOGgG",
        "outputId": "fd7c2f62-02b6-4941-f999-197017495810"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n",
            "/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this action, we can sample states and actions, retrieve rewards and have our agent navigate the frozen lake. "
      ],
      "metadata": {
        "id": "6oIu_KIkOzkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Generating the Q -Table\n",
        "The first step for building the Q - Table is to initiate all the key values to zero for each (state-action) pair.\n",
        "\n",
        "The number of rows in the table is equivalent to the size of the state space in the environment and the number of columns is equivalent to the size of the action space.\n",
        "\n",
        "We can get this information by using the env environment loaded above."
      ],
      "metadata": {
        "id": "sL5nBA5COpmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_space_size = env.action_space.n\n",
        "state_space_size = env.observation_space.n\n",
        "\n",
        "q_table = np.zeros((state_space_size,action_space_size))\n",
        "print(action_space_size,state_space_size)\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY2SsH4GOcOW",
        "outputId": "917b2513-e882-4798-ade3-943ae9dd2184"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 16\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Create and Initialize the parameters needed to implement the Q learning algorithm"
      ],
      "metadata": {
        "id": "EpWDYM8RP6xT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If our trained agent hasn't received to its end goal i.e., the frizbee after the 100th step, then the model will terminate and the total reward of the model will be 0."
      ],
      "metadata": {
        "id": "pUvhVZ1CQdLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set 01\n",
        "count = 1000\n",
        "num_episodes = 4000   #Total episodes\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "lr = 0.001\n",
        "dr = 0.99\n",
        "\n",
        "exploration_rate = .97   # Exploration rate\n",
        "max_exploration_rate = 0.99   # Exploration probability at start\n",
        "min_exploration_rate = 0.0001   # Minimum exploration probability \n",
        "exploration_decay_rate = 0.0001   # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "WV0AvT1RcMKP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set 02\n",
        "count = 1000\n",
        "num_episodes = 10000\n",
        "max_steps_per_episode = 100\n",
        "\n",
        "lr = 0.01\n",
        "dr = 0.999\n",
        "\n",
        "exploration_rate = .95\n",
        "max_exploration_rate = 0.95\n",
        "min_exploration_rate = 0.01\n",
        "exploration_decay_rate = 0.001"
      ],
      "metadata": {
        "id": "Fgv-02zOt8hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list to hold all the rewards we'll get from each episodes\n",
        "rewards_all_episodes = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state = env.reset()   #for each episode, we need to reset the state of the environment\n",
        "\n",
        "  done = False #keeps track of whether or not the episode is finished\n",
        "  rewards_in_current_episode = 0 #keeping track of the rewards within the current episode\n",
        "\n",
        "  #for each time step within an episode\n",
        "  for step in range(max_steps_per_episode):\n",
        "\n",
        "    #exploration+exploitation trade off\n",
        "    exploration_rate_threshold = random.uniform(0,1) #setting the random number\n",
        "    if exploration_rate_threshold > exploration_rate:\n",
        "      action = np.argmax(q_table[state,:]) #exploit and choose the highest value\n",
        "    else:\n",
        "      action = env.action_space.sample()  #agent will explore the environment and sample an action randomly\n",
        "    #tuple\n",
        "    new_state, reward, done, info = env.step(action) #take that action by calling step on env object and pass it through\n",
        "\n",
        "    #Updating the q-table with new values \n",
        "    q_table[state,action] = q_table[state, action] * (1 - lr) + \\\n",
        "    lr * (reward + dr * np.argmax(q_table[new_state,:]))\n",
        "\n",
        "    state = new_state\n",
        "    rewards_in_current_episode += reward\n",
        "\n",
        "    if done == True:\n",
        "      break\n",
        "\n",
        "  #Once episode is finished, we need to update our exploration rate using exponential decay\n",
        "  exploration_rate = min_exploration_rate + \\\n",
        "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
        "  \n",
        "  rewards_all_episodes.append(rewards_in_current_episode)\n",
        "\n",
        "# After all the episodes, we need to calculate the average reward per thousand episodes from our reward list during training\n",
        "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
        "count = 1000\n",
        "print(\"*******************Average Reward Per Thousand Episodes***********************\\n\")\n",
        "for reward_points in rewards_per_thousand_episodes:\n",
        "  print(count, \": \", str(sum(reward_points/1000)))\n",
        "  count += 1000\n",
        "\n",
        "print(\"Total Score over time: \" +  str(sum(reward_points)/num_episodes))\n",
        "#Print the updated Q-table\n",
        "print(\"\\n\\n**********Q-table***********\\n\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-GxAoo9RPCH",
        "outputId": "465a7e47-51ff-4996-c0da-32aa6573f6d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************Average Reward Per Thousand Episodes***********************\n",
            "\n",
            "1000 :  0.011000000000000003\n",
            "2000 :  0.01800000000000001\n",
            "3000 :  0.010000000000000002\n",
            "4000 :  0.01900000000000001\n",
            "Score over time: 0.00475\n",
            "\n",
            "\n",
            "**********Q-table***********\n",
            "\n",
            "[[3.99724979e-01 2.83176000e+00 2.81932644e+00 2.85000000e+00]\n",
            " [2.84911724e+00 2.84912447e+00 2.75512277e+00 2.85000000e+00]\n",
            " [2.84911281e+00 2.84994551e+00 2.84975680e+00 2.85000000e+00]\n",
            " [2.29900584e+00 2.84558738e+00 2.29897666e+00 2.85000000e+00]\n",
            " [4.78653682e-01 3.64800020e-03 1.84472185e+00 3.07653886e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.99189211e+00 3.04000784e-02 3.26496973e-01 2.39016988e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.61168859e+00 1.52146896e+00 1.82522769e+00 4.01523842e-01]\n",
            " [3.22299185e-01 3.66381188e-01 4.50236009e-02 1.95651716e-03]\n",
            " [2.43937434e+00 1.25160604e-02 3.17036941e-01 4.04685302e-07]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.60013644e-01 6.23086503e-02 1.83739183e+00 7.60301587e-01]\n",
            " [1.91773047e+00 1.71806722e+00 9.96864905e-01 1.89152507e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our agent played 4000 episodes. At each time step within an episode the agent received a reward of 1 if it reached the frizbee and otherwise 0.\n",
        "\n",
        "The agent did indeed reach the frizbee then the episode, then the episode finihsed at that time step.\n",
        "\n",
        "So, for the first 1000 episodes we can interpret in this score as meaning 1.1% of the time, the agent received a reward of 1 and won the episode.\n",
        "\n",
        "And by the end, the agent was winning 1.9% of the times. This means the agent increaed a bit it's performance."
      ],
      "metadata": {
        "id": "HfwXa-okaspG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Visualizing the trained agent play Frozen Lake\n",
        "In this section, I'll be trying to visualize how the trained agent is playing in each episode. Therefore, we can see the final output of each episode. "
      ],
      "metadata": {
        "id": "N665KgIRcDrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(3):\n",
        "  state = env.reset()\n",
        "  step = 0\n",
        "  done = False \n",
        "  print(\"***********EPISODE \", episode+1, \"***********\\n\\n\\n\\n\")\n",
        "  time.sleep(1) #making the agent sleep for 1 second to get ready for the current or next episode\n",
        "\n",
        "  for step in range(max_steps_per_episode):\n",
        "    clear_output(wait=True) #clears the output from the current cell to avoid overrite\n",
        "    env.render() #renders on the env object to render the current state of the environment to the display to visually see the game grid & where exactly our agent is on the grid\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    action = np.argmax(q_table[state,:])  #setting action to the highest Q value from the Q-table for our current state\n",
        "    new_state, reward, done, info = env.step(action) # updating the action's correspondng new_state, reward, done or not and important information\n",
        "\n",
        "    if done:\n",
        "      clear_output(wait=True)\n",
        "      env.render()\n",
        "      if reward == 1:\n",
        "        print(\"*************You reached your goal!******\")\n",
        "      else:\n",
        "        print(\"************You fell in a hole! Try Again :( ********\")\n",
        "        time.sleep(1)\n",
        "\n",
        "      clear_output(wait=True)\n",
        "    state= new_state\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZQF4C3JZvnl",
        "outputId": "301ad3dd-4aef-48e8-9928-42582123c011"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************You fell in a hole! Try Again :( ********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above results, for the top 3 episodes, the agent mostly fell into the hole. The agent wasn't able to reach to the final goal. In order to combat this, I believe the next logical step would be fine-tune and experiment with different values of the learning parameters. "
      ],
      "metadata": {
        "id": "KW1wD7VzNiKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Conclusions"
      ],
      "metadata": {
        "id": "slTMxriU63mp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above exercise, I was introduced to the process of training a reinforcement learning agent. I was able to understand the importance of learning rate, decay rate and the exploration rate. It has been really interesting to make an agent be trained to play the FrozenLake. By following the tutorial I was able to carry out this assignment. Although, the future recommendations for the assignment would be to fine - tune it a bit more and increase it's chances of reaching its goal. "
      ],
      "metadata": {
        "id": "cn70_xS0FGZB"
      }
    }
  ]
}